{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets are: Iris, Titanic, Bike, Servo.\n",
      " 1. Decision Stumps \n",
      " 2. Decision Tree \n",
      " 3. Bagging \n",
      " 4. Random Forest\n",
      "Enter a choice:4\n",
      "Which dataset would like the implementation of Random Forest for?bike\n",
      "\n",
      "------------------------------Random Forest For Bike Sharing------------------------------\n",
      "\n",
      "[{'atemp <= 0.39835': [{'windspeed <= 0.129987': [3443.0,\n",
      "                                                  {'weekday = 0': [2466.6666666666665,\n",
      "                                                                   1840.3333333333333]}]},\n",
      "                       {'yr = 1': [{'hum <= 0.738333': [6834.142857142857,\n",
      "                                                        4934.333333333333]},\n",
      "                                   {'atemp <= 0.4564': [2534.5,\n",
      "                                                        4528.571428571428]}]}]},\n",
      " {'yr = 1': [{'atemp <= 0.369938': [3587.5, {'week <= 36': [6754.0, 7919.2]}]},\n",
      "             {'week <= 9': [1255.3333333333333,\n",
      "                            {'week <= 43': [4493.875, 2566.0]}]}]},\n",
      " {'yr = 1': [{'windspeed <= 0.142404': [{'week <= 32': [6563.666666666667,\n",
      "                                                        7764.25]},\n",
      "                                        {'week <= 1': [1951.0,\n",
      "                                                       5412.333333333333]}]},\n",
      "             {'atemp <= 0.451988': [{'hum <= 0.524583': [2846.25, 1418.0]},\n",
      "                                    {'windspeed <= 0.0970208': [1996.0,\n",
      "                                                                4377.0]}]}]},\n",
      " {'yr = 1': [{'atemp <= 0.437488': [5057.0,\n",
      "                                    {'windspeed <= 0.17538299999999998': [7845.0,\n",
      "                                                                          6934.8]}]},\n",
      "             {'week <= 13': [1613.75,\n",
      "                             {'hum <= 0.455417': [5895.0,\n",
      "                                                  3713.4444444444443]}]}]},\n",
      " {'atemp <= 0.611121': [{'atemp <= 0.345317': [{'atemp <= 0.213509': [2402.3333333333335,\n",
      "                                                                      3368.2]},\n",
      "                                               {'windspeed <= 0.0783667': [2395.0,\n",
      "                                                                           4755.818181818182]}]},\n",
      "                        {'is_month_start = True': [4898.5,\n",
      "                                                   7578.666666666667]}]},\n",
      " {'atemp <= 0.44064200000000003': [{'hum <= 0.7575': [{'week <= 12': [2102.3333333333335,\n",
      "                                                                      3723.1666666666665]},\n",
      "                                                      408.5]},\n",
      "                                   {'day_of_month <= 21': [{'week <= 35': [5819.7692307692305,\n",
      "                                                                           7846.0]},\n",
      "                                                           4482.25]}]}]\n",
      "Accuracy for Random Forest is : 0.3704692649528225\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # to calculate mean and standard deviation\n",
    "import pandas as pd  # to load and manipulate data\n",
    "import matplotlib.pyplot as plt # to draw graphs\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "# Import data\n",
    "#----------------------Iris dataset------------------------\n",
    "data_iris = pd.read_csv(\"iris.data\",header = None)   # reading the file and returning a dataframe\n",
    "data_iris.columns = [\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"label\"]    # here slen=sepal length, siwd= sepal width, plen= petal length, pwid= petal width and class\n",
    "\n",
    "\n",
    "#----------------------Titanic dataset------------------------\n",
    "data_titanic = pd.read_csv(\"train.csv\")             #reading the titanic dataset in data_titanic\n",
    "data_titanic[\"Label\"] = data_titanic.Survived       #Data preparation\n",
    "data_titanic = data_titanic.drop([\"PassengerId\",\"Survived\",\"Name\",\"Ticket\",\"Cabin\"],axis = 1)\n",
    "\n",
    "#handling missing values \n",
    "median_age = data_titanic.Age.median()\n",
    "mode_embarked = data_titanic.Embarked.mode()[0]\n",
    "data_titanic = data_titanic.fillna({\"Age\":median_age, \"Embarked\":mode_embarked})\n",
    "\n",
    "\n",
    "#----------------------Bike Sharing dataset------------------------\n",
    "data_bike = pd.read_csv(\"bike.csv\", parse_dates=[\"dteday\"])       #reading the bike dataset in data_bike\n",
    "data_bike = data_bike.drop([\"instant\",\"casual\",\"registered\"], axis=1) #Data preparation\n",
    "data_bike = data_bike.rename({\"dteday\":\"date\"},axis=1)\n",
    "\n",
    "date_column = data_bike.date\n",
    "\n",
    "data_bike[\"day_of_year\"] = date_column.dt.dayofyear\n",
    "data_bike[\"day_of_month\"] = date_column.dt.day\n",
    "\n",
    "data_bike[\"quarter\"] = date_column.dt.quarter\n",
    "data_bike[\"week\"] = date_column.dt.week\n",
    "\n",
    "data_bike[\"is_month_end\"] = date_column.dt.is_month_end\n",
    "data_bike[\"is_month_start\"] = date_column.dt.is_month_start\n",
    "data_bike[\"is_quarter_end\"] = date_column.dt.is_quarter_end\n",
    "data_bike[\"is_quarter_start\"] = date_column.dt.is_quarter_start\n",
    "data_bike[\"is_year_end\"] = date_column.dt.is_year_end\n",
    "data_bike[\"is_year_start\"] = date_column.dt.is_year_end\n",
    "\n",
    "data_bike = data_bike.set_index(\"date\")\n",
    "\n",
    "data_bike[\"label\"] = data_bike.cnt\n",
    "data_bike = data_bike.drop(\"cnt\", axis=1)\n",
    "\n",
    "#----------------------Servo dataset------------------------\n",
    "data_servo=pd.read_csv(\"servo.data\", sep=',', names=[\"motor\",\"screw\",\"p_gain\",\"v_gain\",\"label\"])   #reading the servo dataset in data_servo\n",
    "\n",
    "# this function is used to split the dataset into train and test, and takes 2 arguments: the dataframe and the desired size of the test data\n",
    "def train_and_test(df,test_size):\n",
    "       \n",
    "    to_list = df.index.tolist()             # convert the indexes of the dataframe to a list\n",
    "    test_data = random.sample(to_list,test_size)     # randomly sample k indices for test data\n",
    "    \n",
    "    test_df = df.loc[test_data]\n",
    "    train_df = df.drop(test_data)\n",
    "\n",
    "    return train_df,test_df\n",
    "\n",
    "# This function checks how homogenous is the data and returns a boolean value accordingly\n",
    "\n",
    "def homogeneity(data):\n",
    "    pred_col = data[:,-1]           # take just the label column which is the last(-1) in this case\n",
    "    unique = np.unique(pred_col)   # taking only the unique values \n",
    "\n",
    "    if len(unique) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "#This function classifies the data according to the labels\n",
    "\n",
    "def terminal_node(data, ml_task):\n",
    "    \n",
    "    pred_col = data[:,-1]\n",
    "    if ml_task == \"classification\":\n",
    "        unique_class, counts = np.unique(pred_col, return_counts=True) #take the unique classes and their respective counts \n",
    "\n",
    "        index = counts.argmax()   #max count of which label? then stored in index\n",
    "        node = unique_class[index]   #returns the class of that max value \n",
    "    \n",
    "    #regression\n",
    "    else:\n",
    "        node = np.mean(pred_col)\n",
    "\n",
    "    return node\n",
    "\n",
    "def possible_splits(data, random_subspace=None):\n",
    "    splits = {}\n",
    "    _, n_columns = data.shape  # takes no of columns only\n",
    "    index = list(range(n_columns - 1))\n",
    "    \n",
    "    if random_subspace and random_subspace <= len(index):\n",
    "        index = random.sample(population=index, k=random_subspace)\n",
    "    \n",
    "    for i in index:   \n",
    "        values = data[:,i]           # taking all the values from the iterated column \n",
    "        unique_values = np.unique(values)       # storing the unique values\n",
    "        splits[i] = unique_values\n",
    "                    \n",
    "                    \n",
    "    \n",
    "    return splits\n",
    "\n",
    "# this function splits the data on the specific column(attribute) and on a particular value\n",
    "def splitting(data,column_to_split,value_to_splitat):\n",
    "    split_column_values = data[:,column_to_split]                       #taking only the values of the column\n",
    "    type_of_feature = feature_types[column_to_split]\n",
    "    if type_of_feature == \"categorical\":\n",
    "        data_below = data[split_column_values == value_to_splitat]\n",
    "        data_above = data[split_column_values != value_to_splitat]   \n",
    "        \n",
    "    else:\n",
    "        data_below = data[split_column_values <= value_to_splitat]    # compare the column values with our best/decided value\n",
    "        data_above = data[split_column_values > value_to_splitat]    # and then split them accordingly.\n",
    "        \n",
    "        \n",
    "    return data_below,data_above\n",
    "\n",
    "#this function calculates the mean squared error for continuous values\n",
    "def meansqerror(data):\n",
    "    actual_values = data[:,-1]\n",
    "    if len(actual_values) != 0:  # empty data\n",
    "        prediction = np.mean(actual_values)\n",
    "        mse = np.mean((actual_values - prediction)**2)\n",
    "       \n",
    "    else:\n",
    "        mse = 0\n",
    "\n",
    "    return mse\n",
    "\n",
    "#this function calculates the entropy\n",
    "def entropy(data):\n",
    "    lbl_column = data[:,-1]\n",
    "    _,counts = np.unique(lbl_column, return_counts=True)\n",
    "\n",
    "    probability = counts / counts.sum()\n",
    "    entropy1 = sum(probability * -np.log2(probability))\n",
    "    \n",
    "    return entropy1\n",
    "\n",
    "#below given function calculates the chosen metric i.e mse or entropy for the whole data.\n",
    "def calculate_overall_metric(data_below,data_above, metric_function):\n",
    "    lb = len(data_below)\n",
    "    la = len(data_above)\n",
    "    data_points = lb + la\n",
    "\n",
    "    p_data_below = lb / data_points\n",
    "    p_data_above = la / data_points\n",
    "\n",
    "    overall_metric = (p_data_below * metric_function(data_below)) + (p_data_above * metric_function(data_above))\n",
    "\n",
    "    return overall_metric\n",
    "\n",
    "#This function chooses the best split from the list of potential splits given to it on the basis of the metric function.\n",
    "def best_split(data, potential_splits, ml_task):\n",
    "    first_iteration = True   \n",
    "    for col_index in potential_splits:\n",
    "        for value in potential_splits[col_index]:\n",
    "            data_below, data_above = splitting(data,col_index,value)\n",
    "            if ml_task == \"classification\":\n",
    "                current_metric = calculate_overall_metric(data_below,data_above,metric_function=entropy)\n",
    "            #regression\n",
    "            else:\n",
    "                current_metric = calculate_overall_metric(data_below,data_above,metric_function=meansqerror)\n",
    "\n",
    "            if first_iteration or current_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                best_overall_metric = current_metric\n",
    "                best_split_column = col_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column,best_split_value\n",
    "\n",
    "#This function determines the type of feature whether categorical or continuous.\n",
    "def determine_feature(df):\n",
    "    feature_types = []\n",
    "    threshold = 15\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique_values = df[col].unique()\n",
    "        example = unique_values[0]\n",
    "        if (isinstance(example,str)) or (len(unique_values) <= threshold):\n",
    "            feature_types.append(\"categorical\")\n",
    "        else:\n",
    "            feature_types.append(\"continuous\")\n",
    "            \n",
    "    return feature_types\n",
    "\n",
    "\n",
    "#This is the main decision tree algorithm which prints out the decision tree. It takes 6 parameters,\n",
    "#out of which 4 are set to default. min samples are the number of leaf nodes and max dep is the number of subnodes.\n",
    "\n",
    "def decision_tree_algorithm(df, ml_task, counter=0, min_sample = 2, max_dep = 5, random_subspace = None):\n",
    "    \n",
    "    #data preparation\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, feature_types\n",
    "        COLUMN_HEADERS = df.columns       #to take the actual names of columns\n",
    "        feature_types = determine_feature(df)\n",
    "        data = df.values\n",
    "    \n",
    "    else:\n",
    "        data = df\n",
    "    \n",
    "    #base cases\n",
    "    if (homogeneity(data)) or (len(data) < min_sample) or (counter == max_dep):\n",
    "        node = terminal_node(data, ml_task)\n",
    "        return node\n",
    "    \n",
    "    #recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "        #helper function\n",
    "        splits = possible_splits(data, random_subspace)\n",
    "        split_column, split_value = best_split(data, splits, ml_task)\n",
    "        data_below, data_above = splitting(data,split_column, split_value)\n",
    "        \n",
    "        #instantiate sub_tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        type_of_feature = feature_types[split_column]\n",
    "        \n",
    "        if type_of_feature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(feature_name,split_value)\n",
    "        else:\n",
    "            question = \"{} = {}\".format(feature_name,split_value)\n",
    "        \n",
    "        subtree = {question: []}\n",
    "        \n",
    "        #find answers\n",
    "        if_yes = decision_tree_algorithm(data_below,ml_task, counter, min_sample, max_dep, random_subspace)\n",
    "        if_no = decision_tree_algorithm(data_above,ml_task, counter, min_sample, max_dep, random_subspace)\n",
    "        \n",
    "        if if_yes == if_no:\n",
    "            subtree = if_yes\n",
    "        \n",
    "        else:\n",
    "            subtree[question].append(if_yes)\n",
    "            subtree[question].append(if_no)\n",
    "        \n",
    "        return subtree\n",
    "    \n",
    "#this function gives the implementation of decision stumps i.e decision tree with only 1 subnode. \n",
    "def decision_stumps(df, ml_task):\n",
    "    tree = decision_tree_algorithm(df, ml_task,max_dep = 1, random_subspace=None)\n",
    "    \n",
    "    return tree\n",
    "    \n",
    "#this function predicts any instance\n",
    "def predict_example(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    feature, comp_op, value = question.split(\" \")\n",
    "\n",
    "    #ask question\n",
    "    if comp_op == \"<=\":\n",
    "        if example[feature] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    else:               # categorical feature\n",
    "        if str(example[feature]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "    #base case\n",
    "    if not isinstance(answer,dict):\n",
    "        return answer\n",
    "    #recursive part\n",
    "    else: \n",
    "        residual_tree = answer\n",
    "        return predict_example(example,residual_tree)\n",
    "    \n",
    "    \n",
    "#this function applies the predictions to all the decision tree.\n",
    "\n",
    "def decision_tree_predictions(test_df, tree):\n",
    "    predictions = test_df.apply(predict_example, args=(tree,), axis=1)\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    predictions_correct = predictions == labels\n",
    "    accuracy = predictions_correct.mean()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# ideally r squared should be as close to 1 as possible\n",
    "def r_squared(df, tree):\n",
    "    labels = df.label\n",
    "    mean = labels.mean()\n",
    "    predictions = df.apply(predict_example, args=(tree,),axis=1)\n",
    "    \n",
    "    res = sum((labels - predictions) ** 2)\n",
    "    tot = sum((labels - mean) ** 2)\n",
    "    ans = 1 - res / tot\n",
    "    \n",
    "    return ans\n",
    "\n",
    "def bootstrap_sampling(data, number):\n",
    "    indices = np.random.randint(low=0, high=len(data), size= number)\n",
    "    bootstrapped_data = data.iloc[indices]\n",
    "    \n",
    "    return bootstrapped_data\n",
    "\n",
    "\n",
    "def baggingr(data, no_of_samples, no_of_bags):\n",
    "    #bagged_data = {}\n",
    "    dtree = []\n",
    "    for i in range(no_of_bags):\n",
    "        bootstrap_data = bootstrap_sampling(data, no_of_samples)\n",
    "        tree = decision_tree_algorithm(data, \"regression\", max_dep=3)\n",
    "        dtree.append(tree)\n",
    "        \n",
    "    return dtree\n",
    "        \n",
    "def baggingc(data, no_of_samples, no_of_bags):\n",
    "    #bagged_data = {}\n",
    "    dtree = []\n",
    "    for i in range(no_of_bags):\n",
    "        bootstrap_data = bootstrap_sampling(data, no_of_samples)\n",
    "        tree = decision_tree_algorithm(bootstrap_data, \"classification\", max_dep=3)\n",
    "        dtree.append(tree)\n",
    "        \n",
    "    return dtree\n",
    "\n",
    "def random_forest_clas(train_df, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "    forest = []\n",
    "    for i in range(n_trees):\n",
    "        df_bootstrapped = bootstrap_sampling(train_df, n_bootstrap)\n",
    "        tree = decision_tree_algorithm(df_bootstrapped,\"classification\",0,dt_max_depth,n_features)\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest\n",
    "\n",
    "def random_forest_reg(train_df, n_trees, n_bootstrap, n_features, dt_max_depth):\n",
    "    forest = []\n",
    "    for i in range(n_trees):\n",
    "        df_bootstrapped = bootstrap_sampling(train_df, n_bootstrap)\n",
    "        tree = decision_tree_algorithm(df_bootstrapped,\"regression\",0,dt_max_depth,n_features)\n",
    "        forest.append(tree)\n",
    "    \n",
    "    return forest\n",
    "\n",
    "def rforest_predictions(test_df,forest):\n",
    "    df_predictions = {}\n",
    "    for i in range(len(forest)):\n",
    "        column_name = \"tree_{}\".format(i)\n",
    "        predictions = decision_tree_predictions(test_df, tree=forest[i])\n",
    "        df_predictions[column_name] = predictions\n",
    "        \n",
    "    df_predictions = pd.DataFrame(df_predictions)\n",
    "    random_forest_predictions = df_predictions.mode(axis=1)[0]\n",
    "   \n",
    "    return random_forest_predictions\n",
    "\n",
    "\n",
    "def calculate_r_squared_random(df, forest):\n",
    "    labels = df.label\n",
    "    mean = labels.mean()\n",
    "    df_predictions = []\n",
    "    for i in range(len(forest)):\n",
    "        column_name = \"tree_{}\".format(i)\n",
    "        predictions = decision_tree_predictions(df, tree=forest[i])\n",
    "        ss_res = sum((labels - predictions) ** 2)\n",
    "        ss_tot = sum((labels - mean) ** 2)\n",
    "        r_squared = 1 - ss_res / ss_tot\n",
    "        df_predictions.append(r_squared)\n",
    "        \n",
    "    aq = np.array(df_predictions).mean()\n",
    "\n",
    "    return aq\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "            \n",
    "    print(\"Available datasets are: Iris, Titanic, Bike, Servo.\")\n",
    "            \n",
    "    iris_train, iris_test = train_and_test(data_iris, 20)\n",
    "            \n",
    "    titanic_train, titanic_test = train_and_test(data_titanic, 20)\n",
    "            \n",
    "    \n",
    "    bike_train, bike_test = train_and_test(data_bike, 20)\n",
    "            \n",
    "    servo_train, servo_test = train_and_test(data_servo, 20)\n",
    "                                    \n",
    "    print(\" 1. Decision Stumps \\n 2. Decision Tree \\n 3. Bagging \\n 4. Random Forest\")\n",
    "    choice = int(input(\"Enter a choice:\"))\n",
    "    if choice == 1:\n",
    "        ds = input(\"Which dataset would like the implementation of Decision Stumps for?\")\n",
    "        if ds == \"Iris\" or ds == \"iris\":\n",
    "                    \n",
    "            iris_stump = decision_stumps(iris_train, \"classification\")\n",
    "\n",
    "            print(\"\\n------------------------------Decision Stump For Iris------------------------------\\n\")\n",
    "            pprint(iris_stump)\n",
    "\n",
    "        elif ds ==\"Titanic\" or ds == \"titanic\":\n",
    "\n",
    "            titanic_stump = decision_stumps(titanic_train, \"classification\")\n",
    "\n",
    "            print(\"\\n------------------------------Decision Stump For Titanic------------------------------\\n\")\n",
    "            pprint(titanic_stump)\n",
    "\n",
    "        elif ds == \"Bike\" or ds == \"bike\":\n",
    "\n",
    "            bike_stump = decision_stumps(bike_train, \"regression\")\n",
    "\n",
    "            print(\"\\n------------------------------Decision Stump For Bike Sharing------------------------------\\n\")\n",
    "            pprint(bike_stump)\n",
    "\n",
    "        else:\n",
    "\n",
    "            servo_stump = decision_stumps(servo_train, \"regression\")\n",
    "\n",
    "            print(\"\\n------------------------------Decision Stump For Servo------------------------------\\n\")\n",
    "            pprint(servo_stump)\n",
    "\n",
    "    elif choice == 2:\n",
    "        ds = input(\"Which dataset would like the implementation of Decision Trees for?\")\n",
    "        if ds == \"Iris\" or ds == \"iris\":\n",
    "            iris_tree = decision_tree_algorithm(iris_train, \"classification\")\n",
    "            iris_predictions = decision_tree_predictions(iris_test, iris_tree)\n",
    "            tree_accuracy1 = calculate_accuracy(iris_predictions, iris_test.values[:,-1])\n",
    "\n",
    "            print(\"\\n------------------------------Decision Tree For Iris------------------------------\\n\")\n",
    "            pprint(iris_tree)\n",
    "\n",
    "            print(\"Accuracy for Decision Tree is :\",tree_accuracy1)\n",
    "\n",
    "        elif ds ==\"Titanic\" or ds == \"titanic\":\n",
    "            titanic_tree = decision_tree_algorithm(titanic_train, \"classification\")\n",
    "            titanic_predictions = decision_tree_predictions(titanic_test, titanic_tree)\n",
    "            tree_accuracy2 = calculate_accuracy(titanic_predictions, titanic_test.values[:,-1])\n",
    "\n",
    "            print(\"\\n------------------------------Decision Tree For Titanic------------------------------\\n\")\n",
    "            pprint(titanic_tree)\n",
    "\n",
    "            print(\"Accuracy for Decision Tree is :\",tree_accuracy2)\n",
    "\n",
    "        elif ds == \"Bike\" or ds == \"bike\":\n",
    "            bike_tree = decision_tree_algorithm(bike_train, \"regression\", max_dep=4)\n",
    "            tree_accuracy3 = r_squared(bike_test, bike_tree)\n",
    "\n",
    "            print(\"\\n------------------------------Decision Tree For Bike Sharing------------------------------\\n\")\n",
    "            pprint(bike_tree)\n",
    "\n",
    "            print(\"Accuracy for Decision Tree is :\",tree_accuracy3)\n",
    "\n",
    "        else:\n",
    "            servo_tree = decision_tree_algorithm(servo_train, \"regression\")\n",
    "            tree_accuracy4 = r_squared(servo_test, servo_tree)\n",
    "\n",
    "            print(\"\\n------------------------------Decision Tree For Servo------------------------------\\n\")\n",
    "            pprint(servo_tree)\n",
    "\n",
    "            print(\"Accuracy for Decision Tree is :\",tree_accuracy4)\n",
    "    elif choice == 3:\n",
    "        ds = input(\"Which dataset would like the implementation of Bagging for?\")\n",
    "        if ds == \"Iris\" or ds == \"iris\":\n",
    "            iris_bag = baggingc(iris_train, 25, 4)\n",
    "        \n",
    "            iris_predictions2 = rforest_predictions(iris_test, iris_bag)\n",
    "            bag_accuracy1 = calculate_accuracy(iris_predictions2, iris_test.label)\n",
    "        \n",
    "            print(\"\\n------------------------------Bagging For Iris------------------------------\\n\")\n",
    "            pprint(iris_bag)\n",
    "            print(\"Accuracy for Bagging is :\",bag_accuracy1)\n",
    "        \n",
    "        elif ds ==\"Titanic\" or ds == \"titanic\":\n",
    "            titanic_bag = baggingc(titanic_train, 25, 4)\n",
    "        \n",
    "            titanic_predictions2 = rforest_predictions(titanic_test, titanic_bag)\n",
    "            bag_accuracy2 = calculate_accuracy(titanic_predictions2, titanic_test.Label)\n",
    "        \n",
    "            print(\"\\n------------------------------Bagging For Titanic------------------------------\\n\")\n",
    "            pprint(titanic_bag)\n",
    "            print(\"Accuracy for Bagging is :\",bag_accuracy2)\n",
    "        \n",
    "        \n",
    "        elif ds == \"Bike\" or ds == \"bike\":\n",
    "            bike_bag = baggingr(bike_train, 25, 5)\n",
    "\n",
    "            bag_accuracy3 = calculate_r_squared_random(bike_test, bike_bag)\n",
    "\n",
    "            print(\"\\n------------------------------Bagging For Bike Sharing------------------------------\\n\")\n",
    "            pprint(bike_bag)\n",
    "            print(\"Accuracy for Bagging is :\",bag_accuracy3)\n",
    "\n",
    "        else:\n",
    "            servo_bag = baggingr(servo_train, 25, 4)\n",
    "\n",
    "            bag_accuracy4 = calculate_r_squared_random(servo_test, servo_bag)\n",
    "\n",
    "            print(\"\\n------------------------------Bagging For Servo------------------------------\\n\")\n",
    "            pprint(servo_bag)\n",
    "            print(\"Accuracy for Bagging is :\",bag_accuracy4)\n",
    "            \n",
    "    else:\n",
    "        ds = input(\"Which dataset would like the implementation of Random Forest for?\")\n",
    "        if ds == \"Iris\" or ds == \"iris\":\n",
    "            iris_forest = random_forest_clas(iris_train,3,30,3,5)\n",
    "\n",
    "            iris_predictions3 = rforest_predictions(iris_test, iris_forest)\n",
    "            forest_accuracy1 = calculate_accuracy(iris_predictions3, iris_test.label)\n",
    "\n",
    "            print(\"\\n------------------------------Random Forest For Iris------------------------------\\n\")\n",
    "            pprint(iris_forest)\n",
    "            print(\"Accuracy for Random Forest is :\",forest_accuracy1)\n",
    "        \n",
    "        \n",
    "        elif ds ==\"Titanic\" or ds == \"titanic\":\n",
    "            titanic_forest = random_forest_clas(titanic_train,3,30,3,5)\n",
    "\n",
    "            titanic_predictions3 = rforest_predictions(titanic_test, titanic_forest)\n",
    "            forest_accuracy2 = calculate_accuracy(titanic_predictions3, titanic_test.Label)\n",
    "\n",
    "\n",
    "            print(\"\\n------------------------------Random Forest For Titanic------------------------------\\n\")\n",
    "            pprint(titanic_forest)\n",
    "            print(\"Accuracy for Random Forest is :\",forest_accuracy2)\n",
    "            \n",
    "        elif ds == \"Bike\" or ds == \"bike\":\n",
    "            bike_forest = random_forest_reg(bike_train,6,30,3,5)\n",
    "\n",
    "            forest_accuracy3 = calculate_r_squared_random(bike_test, bike_forest)\n",
    "\n",
    "            print(\"\\n------------------------------Random Forest For Bike Sharing------------------------------\\n\")\n",
    "            pprint(bike_forest)\n",
    "            print(\"Accuracy for Random Forest is :\",forest_accuracy3)\n",
    "        else:\n",
    "            servo_forest = random_forest_reg(servo_train,3,30,3,5)\n",
    "        \n",
    "\n",
    "            forest_accuracy4 = calculate_r_squared_random(servo_test, servo_forest)\n",
    "\n",
    "            print(\"\\n------------------------------Random Forest For Servo------------------------------\\n\")\n",
    "            pprint(servo_forest)\n",
    "            print(\"Accuracy for Random Forest is :\",forest_accuracy4)\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
